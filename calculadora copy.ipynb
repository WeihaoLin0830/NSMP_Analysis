{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Individual Risk & Profile Calculator (RSF + Cluster/Subcluster)\n",
        "\n",
        "This notebook loads **already trained** artifacts exported from your training notebooks and provides:\n",
        "\n",
        "- **Clinical profile**: `cluster_pred` (1/2) and, if `cluster_pred==1`, `subcluster_pred` (e.g., 11/12) + `p_subcluster11` + `threshold_used`.\n",
        "- **Individual survival** (RSF): `S_1y`, `S_3y`, `S_5y` at 365/1095/1825 days, plus `risk_3y = 1 - S_3y`.\n",
        "- **Risk group**: `bajo/medio/alto` using terciles of `risk_3y` loaded from `artifact/risk_cutpoints.json`.\n",
        "- **Quality/confidence warnings**: `many_missing_flag`, `low_confidence_flag`, `unknown_category_flag`.\n",
        "\n",
        "**No retraining and no recalibration** happens here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
            "numpy: 2.2.6\n",
            "pandas: 2.3.3\n",
            "sklearn: 1.7.2\n",
            "joblib: 1.5.2\n"
          ]
        }
      ],
      "source": [
        "# Imports & environment info\n",
        "import json\n",
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('numpy:', np.__version__)\n",
        "print('pandas:', pd.__version__)\n",
        "print('sklearn:', sklearn.__version__)\n",
        "print('joblib:', joblib.__version__)\n",
        "\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load_artifacts",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded optional: artifact/risk_model_cox.joblib\n",
            "Loaded: artifact/feature_schema.json\n",
            "Loaded: artifact/risk_model_rsf.joblib\n",
            "Loaded: artifact/cluster_assigner.joblib\n",
            "Loaded: artifact/subcluster1_assigner.joblib\n",
            "Loaded: artifact/subcluster1_label_map.json\n",
            "Loaded: artifact/risk_cutpoints.json\n",
            "\n",
            "Schema summary:\n",
            "  n_expected_columns: 22\n",
            "  n_num_cols: 6\n",
            "  n_cat_cols: 10\n",
            "  n_flag_cols: 6\n",
            "  horizons_days: [365, 1095, 1825]\n"
          ]
        }
      ],
      "source": [
        "# Load artifacts\n",
        "\n",
        "ARTIFACT_DIR = Path('artifact')\n",
        "\n",
        "PATHS = {\n",
        "    'feature_schema': ARTIFACT_DIR / 'feature_schema.json',\n",
        "    'risk_model_rsf': ARTIFACT_DIR / 'risk_model_rsf.joblib',\n",
        "    'risk_model_cox': ARTIFACT_DIR / 'risk_model_cox.joblib',  # optional\n",
        "    'cluster_assigner': ARTIFACT_DIR / 'cluster_assigner.joblib',\n",
        "    'subcluster1_assigner': ARTIFACT_DIR / 'subcluster1_assigner.joblib',\n",
        "    'subcluster1_label_map': ARTIFACT_DIR / 'subcluster1_label_map.json',\n",
        "    'subcluster1_summary': ARTIFACT_DIR / 'subcluster1_summary.json',  # optional\n",
        "    'train_summary': ARTIFACT_DIR / 'train_summary.json',  # optional\n",
        "    'risk_cutpoints': ARTIFACT_DIR / 'risk_cutpoints.json',  # recommended for risk_group\n",
        "}\n",
        "\n",
        "required_keys = [\n",
        "    'feature_schema',\n",
        "    'risk_model_rsf',\n",
        "    'cluster_assigner',\n",
        "    'subcluster1_assigner',\n",
        "    'subcluster1_label_map',\n",
        "]\n",
        "\n",
        "missing_required = [k for k in required_keys if not PATHS[k].exists()]\n",
        "if missing_required:\n",
        "    raise FileNotFoundError(\n",
        "        'Missing required artifacts: '\n",
        "        + ', '.join(f'{k} -> {PATHS[k]}' for k in missing_required)\n",
        "        + '. Run model.ipynb export cells (and subgroups.ipynb export cells) first.'\n",
        "    )\n",
        "\n",
        "with open(PATHS['feature_schema'], 'r', encoding='utf-8') as f:\n",
        "    FEATURE_SCHEMA = json.load(f)\n",
        "\n",
        "EXPECTED_COLUMNS = FEATURE_SCHEMA['expected_columns']\n",
        "NUM_COLS = FEATURE_SCHEMA['num_cols']\n",
        "CAT_COLS = FEATURE_SCHEMA['cat_cols']\n",
        "FLAG_COLS = FEATURE_SCHEMA['flag_cols']\n",
        "HORIZONS_DAYS = FEATURE_SCHEMA.get('horizons_days', [365, 1095, 1825])\n",
        "\n",
        "risk_model_rsf = joblib.load(PATHS['risk_model_rsf'])\n",
        "cluster_assigner = joblib.load(PATHS['cluster_assigner'])\n",
        "subcluster1_assigner = joblib.load(PATHS['subcluster1_assigner'])\n",
        "\n",
        "risk_model_cox = None\n",
        "if PATHS['risk_model_cox'].exists():\n",
        "    try:\n",
        "        risk_model_cox = joblib.load(PATHS['risk_model_cox'])\n",
        "        print('Loaded optional:', PATHS['risk_model_cox'])\n",
        "    except Exception as e:\n",
        "        print('Warning: could not load risk_model_cox.joblib:', repr(e))\n",
        "\n",
        "with open(PATHS['subcluster1_label_map'], 'r', encoding='utf-8') as f:\n",
        "    SUBCLUSTER1_LABEL_MAP = json.load(f)\n",
        "\n",
        "print('Loaded:', PATHS['feature_schema'])\n",
        "print('Loaded:', PATHS['risk_model_rsf'])\n",
        "print('Loaded:', PATHS['cluster_assigner'])\n",
        "print('Loaded:', PATHS['subcluster1_assigner'])\n",
        "print('Loaded:', PATHS['subcluster1_label_map'])\n",
        "if PATHS['risk_cutpoints'].exists():\n",
        "    print('Loaded:', PATHS['risk_cutpoints'])\n",
        "else:\n",
        "    print('Warning: missing', PATHS['risk_cutpoints'], '-> risk_group will be None')\n",
        "\n",
        "# Optional: compare versions against the saved subcluster summary (if present)\n",
        "if PATHS['subcluster1_summary'].exists():\n",
        "    try:\n",
        "        with open(PATHS['subcluster1_summary'], 'r', encoding='utf-8') as f:\n",
        "            _sub_sum = json.load(f)\n",
        "        saved = _sub_sum.get('versions', {})\n",
        "        if saved:\n",
        "            if saved.get('numpy') and saved['numpy'] != np.__version__:\n",
        "                print(f\"Warning: numpy version differs (saved={saved['numpy']} current={np.__version__})\")\n",
        "            if saved.get('sklearn') and saved['sklearn'] != sklearn.__version__:\n",
        "                print(f\"Warning: sklearn version differs (saved={saved['sklearn']} current={sklearn.__version__})\")\n",
        "    except Exception as e:\n",
        "        print('Warning: could not parse subcluster1_summary.json:', repr(e))\n",
        "\n",
        "print()\n",
        "print('Schema summary:')\n",
        "print('  n_expected_columns:', len(EXPECTED_COLUMNS))\n",
        "print('  n_num_cols:', len(NUM_COLS))\n",
        "print('  n_cat_cols:', len(CAT_COLS))\n",
        "print('  n_flag_cols:', len(FLAG_COLS))\n",
        "print('  horizons_days:', HORIZONS_DAYS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "preprocess_introspection",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical default values (first 5):\n",
            "  asa: Missing\n",
            "  histo_defin: Unknown\n",
            "  grado_histologi: Unknown\n",
            "  FIGO2023: Missing\n",
            "  afectacion_linf: Missing\n"
          ]
        }
      ],
      "source": [
        "# Preprocessor introspection utilities\n",
        "# - Decide defaults for missing categorical columns (\"Missing\" if seen by OHE, otherwise \"Unknown\")\n",
        "# - Detect unknown categories (optional warning)\n",
        "\n",
        "def _get_preprocess_from_pipeline(pipeline):\n",
        "    if not hasattr(pipeline, 'named_steps'):\n",
        "        return None\n",
        "    if 'preprocess' in pipeline.named_steps:\n",
        "        return pipeline.named_steps['preprocess']\n",
        "    return None\n",
        "\n",
        "\n",
        "def _get_ohe_from_preprocess(preprocess):\n",
        "    \"\"\"Try to retrieve the fitted OneHotEncoder used for categorical columns.\"\"\"\n",
        "    if preprocess is None:\n",
        "        return None\n",
        "\n",
        "    # ColumnTransformer usually has named_transformers_ after fit.\n",
        "    if hasattr(preprocess, 'named_transformers_') and isinstance(getattr(preprocess, 'named_transformers_'), dict):\n",
        "        return preprocess.named_transformers_.get('cat')\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _build_known_categories_map(preprocess, cat_cols):\n",
        "    \"\"\"Return {cat_col: set(known_categories)} if possible; otherwise None.\"\"\"\n",
        "    ohe = _get_ohe_from_preprocess(preprocess)\n",
        "    if ohe is None:\n",
        "        return None\n",
        "    if not hasattr(ohe, 'categories_'):\n",
        "        return None\n",
        "\n",
        "    cats = list(ohe.categories_)\n",
        "    if len(cats) != len(cat_cols):\n",
        "        print('Warning: OHE categories_ length != len(cat_cols). Unknown-category checks may be unreliable.')\n",
        "\n",
        "    known = {}\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        if i >= len(cats):\n",
        "            break\n",
        "        known[col] = set(str(x) for x in cats[i].tolist())\n",
        "    return known\n",
        "\n",
        "\n",
        "# Prefer to use the cluster_assigner preprocessor (same schema) for category inspection.\n",
        "_preprocess_for_checks = _get_preprocess_from_pipeline(cluster_assigner)\n",
        "KNOWN_CATEGORIES = _build_known_categories_map(_preprocess_for_checks, CAT_COLS)\n",
        "\n",
        "if KNOWN_CATEGORIES is None:\n",
        "    print(\n",
        "        'Note: could not inspect OneHotEncoder categories_. '\n",
        "        'unknown_category_flag will be always False. '\n",
        "        'This is safe because the encoder is configured with handle_unknown=\"ignore\".'\n",
        "    )\n",
        "\n",
        "CAT_DEFAULT_BY_COL = {}\n",
        "if KNOWN_CATEGORIES is not None:\n",
        "    for c in CAT_COLS:\n",
        "        CAT_DEFAULT_BY_COL[c] = 'Missing' if 'Missing' in KNOWN_CATEGORIES.get(c, set()) else 'Unknown'\n",
        "else:\n",
        "    CAT_DEFAULT_BY_COL = {c: 'Unknown' for c in CAT_COLS}\n",
        "\n",
        "print('Categorical default values (first 5):')\n",
        "for c in CAT_COLS[:5]:\n",
        "    print(f'  {c}: {CAT_DEFAULT_BY_COL[c]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "alignment_helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input alignment helpers\n",
        "\n",
        "def build_dataframe_from_dict(patient_dict: dict) -> pd.DataFrame:\n",
        "    \"\"\"Build a single-row DataFrame from a patient dict (no validation yet).\"\"\"\n",
        "    if not isinstance(patient_dict, dict):\n",
        "        raise TypeError('patient_dict must be a dict')\n",
        "    return pd.DataFrame([patient_dict])\n",
        "\n",
        "\n",
        "def align_dataframe(\n",
        "    df_in: pd.DataFrame,\n",
        "    expected_columns: list[str] = EXPECTED_COLUMNS,\n",
        "    num_cols: list[str] = NUM_COLS,\n",
        "    cat_cols: list[str] = CAT_COLS,\n",
        "    flag_cols: list[str] = FLAG_COLS,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Align a batch DataFrame to the expected schema.\n",
        "\n",
        "    Rules:\n",
        "    - Reorder columns to expected_columns\n",
        "    - If missing expected column:\n",
        "        - numeric -> 0.0\n",
        "        - categorical -> \"Missing\" if known, else \"Unknown\"\n",
        "        - flags -> 0\n",
        "    - Extra columns are ignored\n",
        "    \"\"\"\n",
        "    if not isinstance(df_in, pd.DataFrame):\n",
        "        raise TypeError('df_in must be a pandas DataFrame')\n",
        "\n",
        "    df = df_in.copy()\n",
        "\n",
        "    # Add missing columns\n",
        "    for col in expected_columns:\n",
        "        if col in df.columns:\n",
        "            continue\n",
        "\n",
        "        if col in num_cols:\n",
        "            df[col] = 0.0\n",
        "        elif col in cat_cols:\n",
        "            df[col] = CAT_DEFAULT_BY_COL.get(col, 'Unknown')\n",
        "        elif col in flag_cols:\n",
        "            df[col] = 0\n",
        "        else:\n",
        "            df[col] = 0\n",
        "\n",
        "    # Keep only expected columns, in order\n",
        "    df = df.loc[:, expected_columns]\n",
        "\n",
        "    # Normalize dtypes (robust for calculator inputs)\n",
        "    for col in num_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype(float)\n",
        "\n",
        "    for col in flag_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].astype(object)\n",
        "        df[col] = df[col].where(df[col].notna(), CAT_DEFAULT_BY_COL.get(col, 'Unknown'))\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rsf_helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RSF survival prediction helpers\n",
        "\n",
        "def _step_fn_value_at(step_fn, t_days: float) -> float:\n",
        "    \"\"\"Evaluate a scikit-survival StepFunction at horizon t_days.\n",
        "\n",
        "    Strategy:\n",
        "    - If possible, clamp t to the maximum supported time in the step function\n",
        "    - Fall back to a manual 'last time <= t' rule using (x,y)\n",
        "    \"\"\"\n",
        "    t = float(t_days)\n",
        "\n",
        "    if hasattr(step_fn, 'x'):\n",
        "        try:\n",
        "            x = np.asarray(step_fn.x, dtype=float)\n",
        "            if x.size > 0:\n",
        "                t = min(t, float(x.max()))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        return float(step_fn(t))\n",
        "    except Exception:\n",
        "        if hasattr(step_fn, 'x') and hasattr(step_fn, 'y'):\n",
        "            x = np.asarray(step_fn.x, dtype=float)\n",
        "            y = np.asarray(step_fn.y, dtype=float)\n",
        "            if x.size == 0 or y.size == 0:\n",
        "                return float('nan')\n",
        "            idx = int(np.searchsorted(x, t, side='right') - 1)\n",
        "            idx = max(0, min(idx, y.size - 1))\n",
        "            return float(y[idx])\n",
        "        raise\n",
        "\n",
        "\n",
        "def rsf_predict_survival_at_horizons(\n",
        "    X_aligned: pd.DataFrame,\n",
        "    horizons_days: list[int] = HORIZONS_DAYS,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Return S(t) at requested horizons for each row in X_aligned.\"\"\"\n",
        "    preprocess = risk_model_rsf.named_steps['preprocess']\n",
        "    model = risk_model_rsf.named_steps['model']\n",
        "\n",
        "    X_t = preprocess.transform(X_aligned)\n",
        "    sf_list = model.predict_survival_function(X_t)\n",
        "\n",
        "    out = {\n",
        "        'S_1y': [],\n",
        "        'S_3y': [],\n",
        "        'S_5y': [],\n",
        "        'risk_3y': [],\n",
        "    }\n",
        "\n",
        "    t1, t3, t5 = horizons_days\n",
        "\n",
        "    for sf in sf_list:\n",
        "        s1 = _step_fn_value_at(sf, t1)\n",
        "        s3 = _step_fn_value_at(sf, t3)\n",
        "        s5 = _step_fn_value_at(sf, t5)\n",
        "        out['S_1y'].append(s1)\n",
        "        out['S_3y'].append(s3)\n",
        "        out['S_5y'].append(s5)\n",
        "        out['risk_3y'].append(float(1.0 - s3))\n",
        "\n",
        "    return pd.DataFrame(out, index=X_aligned.index)\n",
        "\n",
        "\n",
        "def cox_predict_survival_at_horizons(\n",
        "    X_aligned: pd.DataFrame,\n",
        "    horizons_days: list[int] = HORIZONS_DAYS,\n",
        ") -> pd.DataFrame | None:\n",
        "    \"\"\"Optional baseline (if risk_model_cox is available).\"\"\"\n",
        "    if risk_model_cox is None:\n",
        "        return None\n",
        "\n",
        "    preprocess = risk_model_cox.named_steps['preprocess']\n",
        "    model = risk_model_cox.named_steps['model']\n",
        "\n",
        "    X_t = preprocess.transform(X_aligned)\n",
        "    sf_list = model.predict_survival_function(X_t)\n",
        "\n",
        "    t1, t3, t5 = horizons_days\n",
        "\n",
        "    out = {\n",
        "        'S_1y_cox': [],\n",
        "        'S_3y_cox': [],\n",
        "        'S_5y_cox': [],\n",
        "    }\n",
        "\n",
        "    for sf in sf_list:\n",
        "        out['S_1y_cox'].append(_step_fn_value_at(sf, t1))\n",
        "        out['S_3y_cox'].append(_step_fn_value_at(sf, t3))\n",
        "        out['S_5y_cox'].append(_step_fn_value_at(sf, t5))\n",
        "\n",
        "    return pd.DataFrame(out, index=X_aligned.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "risk_group_reference",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded risk cutpoints from: artifact/risk_cutpoints.json\n",
            "  q33: 0.10676842322737941\n",
            "  q66: 0.32160908694597884\n"
          ]
        }
      ],
      "source": [
        "# Risk-group thresholds (terciles) from precomputed cutpoints (recommended for production)\n",
        "#\n",
        "# In production you should NOT load any patient CSV to compute these.\n",
        "# Instead, model.ipynb exports artifact/risk_cutpoints.json with two numbers:\n",
        "#   - q33: 33% quantile of risk_3y on the training set\n",
        "#   - q66: 66% quantile of risk_3y on the training set\n",
        "\n",
        "RISK_TERCILES: tuple[float, float] | None = None\n",
        "\n",
        "cutpoints_path = PATHS.get('risk_cutpoints', ARTIFACT_DIR / 'risk_cutpoints.json')\n",
        "if cutpoints_path.exists():\n",
        "    with open(cutpoints_path, 'r', encoding='utf-8') as f:\n",
        "        cutpoints = json.load(f)\n",
        "\n",
        "    q33 = float(cutpoints['q33'])\n",
        "    q66 = float(cutpoints['q66'])\n",
        "    RISK_TERCILES = (q33, q66)\n",
        "\n",
        "    print('Loaded risk cutpoints from:', cutpoints_path)\n",
        "    print('  q33:', q33)\n",
        "    print('  q66:', q66)\n",
        "else:\n",
        "    print('Warning: risk_cutpoints.json not found -> risk_group will be None.')\n",
        "    print('Generate it by running model.ipynb export cells to create:', cutpoints_path)\n",
        "\n",
        "\n",
        "def risk_group_from_risk_3y(risk_3y: float, terciles: tuple[float, float] | None = RISK_TERCILES) -> str | None:\n",
        "    if terciles is None or not np.isfinite(risk_3y):\n",
        "        return None\n",
        "    q33, q66 = terciles\n",
        "    if risk_3y <= q33:\n",
        "        return 'bajo'\n",
        "    if risk_3y <= q66:\n",
        "        return 'medio'\n",
        "    return 'alto'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "warnings_helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Warning helpers\n",
        "\n",
        "MISS_COLS = [c for c in EXPECTED_COLUMNS if c.endswith('_miss')]\n",
        "\n",
        "\n",
        "def compute_many_missing_flag(\n",
        "    X_aligned: pd.DataFrame,\n",
        "    missing_threshold: int = 2,\n",
        "    ratio_threshold: float = 0.67,\n",
        ") -> pd.Series:\n",
        "    \"\"\"Return a boolean Series per row.\"\"\"\n",
        "    if not MISS_COLS:\n",
        "        return pd.Series(False, index=X_aligned.index)\n",
        "\n",
        "    miss_sum = X_aligned[MISS_COLS].sum(axis=1)\n",
        "    miss_ratio = miss_sum / float(len(MISS_COLS))\n",
        "\n",
        "    return (miss_sum >= missing_threshold) | (miss_ratio >= ratio_threshold)\n",
        "\n",
        "\n",
        "def compute_unknown_category_flag(X_aligned: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Return a boolean Series per row.\n",
        "\n",
        "    If OHE categories are not accessible, returns all False.\n",
        "    \"\"\"\n",
        "    if KNOWN_CATEGORIES is None:\n",
        "        return pd.Series(False, index=X_aligned.index)\n",
        "\n",
        "    flags = pd.Series(False, index=X_aligned.index)\n",
        "    for col in CAT_COLS:\n",
        "        known = KNOWN_CATEGORIES.get(col)\n",
        "        if not known:\n",
        "            continue\n",
        "        vals = X_aligned[col].astype(str)\n",
        "        flags = flags | (~vals.isin(known))\n",
        "\n",
        "    return flags\n",
        "\n",
        "\n",
        "def compute_low_confidence_flag(p_subcluster11: float | None, margin: float = 0.10) -> bool:\n",
        "    if p_subcluster11 is None or not np.isfinite(p_subcluster11):\n",
        "        return False\n",
        "    return abs(float(p_subcluster11) - 0.5) < float(margin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "assign_profile",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster + subcluster assignment\n",
        "\n",
        "CLUSTER_CONFIDENCE_MARGIN = 0.10\n",
        "\n",
        "\n",
        "def _get_final_estimator(pipeline):\n",
        "    if hasattr(pipeline, 'named_steps'):\n",
        "        for key in ['clf', 'model', 'classifier', 'estimator']:\n",
        "            if key in pipeline.named_steps:\n",
        "                return pipeline.named_steps[key]\n",
        "        return pipeline.steps[-1][1]\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def assign_cluster_and_subcluster(\n",
        "    X_aligned: pd.DataFrame,\n",
        "    threshold: float = 0.5,\n",
        "    margin: float = 0.10,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Assign cluster (1/2) and, if cluster==1, assign subcluster within cluster 1.\n",
        "\n",
        "    Also exposes cluster confidence:\n",
        "    - p_cluster1 = P(cluster==1)\n",
        "    - low_confidence_cluster = |p_cluster1 - 0.5| < 0.10\n",
        "\n",
        "    Note: `margin` controls the subcluster low-confidence flag, not the cluster one.\n",
        "    \"\"\"\n",
        "\n",
        "    out = pd.DataFrame(index=X_aligned.index)\n",
        "\n",
        "    # --- Cluster prediction ---\n",
        "    out['cluster_pred'] = cluster_assigner.predict(X_aligned).astype(int)\n",
        "\n",
        "    # --- Cluster confidence (probability of cluster==1) ---\n",
        "    out['p_cluster1'] = np.nan\n",
        "    out['low_confidence_cluster'] = False\n",
        "\n",
        "    if hasattr(cluster_assigner, 'predict_proba'):\n",
        "        try:\n",
        "            cluster_clf = _get_final_estimator(cluster_assigner)\n",
        "            if not hasattr(cluster_clf, 'classes_'):\n",
        "                raise ValueError('cluster_assigner classifier does not expose classes_.')\n",
        "\n",
        "            classes = np.asarray(cluster_clf.classes_)\n",
        "            idx_1 = np.where(classes == 1)[0]\n",
        "            if idx_1.size == 0:\n",
        "                raise ValueError(f'cluster_assigner classes_ does not include label 1: {classes.tolist()}')\n",
        "\n",
        "            proba = cluster_assigner.predict_proba(X_aligned)\n",
        "            p_cluster1 = proba[:, int(idx_1[0])].astype(float)\n",
        "            out['p_cluster1'] = p_cluster1\n",
        "\n",
        "            p = out['p_cluster1'].astype(float)\n",
        "            out['low_confidence_cluster'] = p.notna() & (np.abs(p - 0.5) < CLUSTER_CONFIDENCE_MARGIN)\n",
        "        except Exception as e:\n",
        "            print('Warning: could not compute p_cluster1 / low_confidence_cluster:', repr(e))\n",
        "    else:\n",
        "        print(\n",
        "            'Note: cluster_assigner has no predict_proba(); '\n",
        "            'p_cluster1 will be NaN and low_confidence_cluster will be False.'\n",
        "        )\n",
        "\n",
        "    # --- Default subcluster outputs ---\n",
        "    out['subcluster_pred'] = pd.Series([None] * len(out), index=out.index, dtype=object)\n",
        "    out['p_subcluster11'] = np.nan\n",
        "    out['threshold_used'] = np.nan\n",
        "    out['low_confidence_flag'] = False\n",
        "\n",
        "    # If not in cluster 1, subcluster assignment is not applicable.\n",
        "    idx_cluster1 = out['cluster_pred'] == 1\n",
        "    if not idx_cluster1.any():\n",
        "        return out\n",
        "\n",
        "    # Identify which model class corresponds to original label 11\n",
        "    o2m = SUBCLUSTER1_LABEL_MAP.get('original_to_model', {})\n",
        "    orig_labels_sorted = SUBCLUSTER1_LABEL_MAP.get('original_labels_sorted')\n",
        "\n",
        "    if '11' not in o2m:\n",
        "        raise ValueError('subcluster1_label_map.json does not contain a mapping for original label \"11\".')\n",
        "\n",
        "    model_label_11 = int(o2m['11'])\n",
        "\n",
        "    sub_clf = _get_final_estimator(subcluster1_assigner)\n",
        "    if not hasattr(sub_clf, 'classes_'):\n",
        "        raise ValueError('subcluster1_assigner classifier does not expose classes_.')\n",
        "\n",
        "    sub_classes = np.asarray(sub_clf.classes_)\n",
        "    col_idx = int(np.where(sub_classes == model_label_11)[0][0])\n",
        "\n",
        "    sub_proba = subcluster1_assigner.predict_proba(X_aligned.loc[idx_cluster1])\n",
        "    p11 = sub_proba[:, col_idx].astype(float)\n",
        "\n",
        "    out.loc[idx_cluster1, 'p_subcluster11'] = p11\n",
        "    out.loc[idx_cluster1, 'threshold_used'] = float(threshold)\n",
        "\n",
        "    # Determine the alternative original label (assumes binary within cluster 1)\n",
        "    if orig_labels_sorted is None:\n",
        "        orig_labels_sorted = sorted(int(k) for k in o2m.keys())\n",
        "\n",
        "    orig_labels_sorted = [int(x) for x in orig_labels_sorted]\n",
        "    if 11 not in orig_labels_sorted:\n",
        "        raise ValueError(f'Expected original label 11 in original_labels_sorted; got {orig_labels_sorted}')\n",
        "\n",
        "    if len(orig_labels_sorted) != 2:\n",
        "        # Fallback to argmax if not binary\n",
        "        pred_model = subcluster1_assigner.predict(X_aligned.loc[idx_cluster1]).astype(int)\n",
        "        m2o = SUBCLUSTER1_LABEL_MAP.get('model_to_original', {})\n",
        "        out.loc[idx_cluster1, 'subcluster_pred'] = [int(m2o.get(str(v), v)) for v in pred_model]\n",
        "    else:\n",
        "        other_label = next(lbl for lbl in orig_labels_sorted if lbl != 11)\n",
        "        out.loc[idx_cluster1, 'subcluster_pred'] = np.where(p11 >= float(threshold), 11, other_label)\n",
        "\n",
        "    # Confidence flag for subcluster assignment\n",
        "    out.loc[idx_cluster1, 'low_confidence_flag'] = [compute_low_confidence_flag(v, margin=margin) for v in p11]\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "predict_one",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main: predict_one\n",
        "\n",
        "def predict_one(\n",
        "    patient_dict: dict,\n",
        "    threshold: float = 0.5,\n",
        "    margin: float = 0.10,\n",
        "    missing_threshold: int = 3,\n",
        ") -> dict:\n",
        "    \"\"\"Predict profile + RSF survival for a single patient dict.\"\"\"\n",
        "\n",
        "    df_one = build_dataframe_from_dict(patient_dict)\n",
        "    X_one = align_dataframe(df_one)\n",
        "\n",
        "    # Warnings based on inputs only\n",
        "    many_missing_flag = bool(compute_many_missing_flag(X_one, missing_threshold=missing_threshold).iloc[0])\n",
        "    unknown_category_flag = bool(compute_unknown_category_flag(X_one).iloc[0])\n",
        "\n",
        "    profile = assign_cluster_and_subcluster(X_one, threshold=threshold, margin=margin).iloc[0].to_dict()\n",
        "    surv = rsf_predict_survival_at_horizons(X_one).iloc[0].to_dict()\n",
        "\n",
        "    risk_group = risk_group_from_risk_3y(float(surv['risk_3y']))\n",
        "\n",
        "    result = {\n",
        "        'cluster_pred': int(profile['cluster_pred']),\n",
        "        'p_cluster1': None if pd.isna(profile.get('p_cluster1', np.nan)) else float(profile['p_cluster1']),\n",
        "        'low_confidence_cluster': bool(profile.get('low_confidence_cluster', False)),\n",
        "        'subcluster_pred': profile['subcluster_pred'],\n",
        "        'p_subcluster11': None if pd.isna(profile['p_subcluster11']) else float(profile['p_subcluster11']),\n",
        "        'threshold_used': None if pd.isna(profile['threshold_used']) else float(profile['threshold_used']),\n",
        "        'S_1y': float(surv['S_1y']),\n",
        "        'S_3y': float(surv['S_3y']),\n",
        "        'S_5y': float(surv['S_5y']),\n",
        "        'risk_3y': float(surv['risk_3y']),\n",
        "        'risk_group': risk_group,\n",
        "        'warnings': {\n",
        "            'many_missing_flag': many_missing_flag,\n",
        "            'low_confidence_cluster': bool(profile.get('low_confidence_cluster', False)),\n",
        "            'low_confidence_flag': bool(profile.get('low_confidence_flag', False)),\n",
        "            'unknown_category_flag': unknown_category_flag,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Human-readable output\n",
        "    print()\n",
        "    print('=== Patient prediction ===')\n",
        "    print('cluster_pred:', result['cluster_pred'])\n",
        "    print('p_cluster1:', result['p_cluster1'])\n",
        "    print('low_confidence_cluster:', result['low_confidence_cluster'])\n",
        "\n",
        "    if result['cluster_pred'] == 1:\n",
        "        print('subcluster_pred:', result['subcluster_pred'])\n",
        "        print('p_subcluster11:', result['p_subcluster11'])\n",
        "        print('threshold_used:', result['threshold_used'])\n",
        "\n",
        "    print(f\"S(1y)={result['S_1y']:.3f} | S(3y)={result['S_3y']:.3f} | S(5y)={result['S_5y']:.3f}\")\n",
        "    print(f\"risk_3y={result['risk_3y']:.3f} | risk_group={result['risk_group']}\")\n",
        "\n",
        "    if any(result['warnings'].values()):\n",
        "        print('Warnings:', result['warnings'])\n",
        "    else:\n",
        "        print('Warnings: none')\n",
        "\n",
        "    if unknown_category_flag:\n",
        "        print('Note: OneHotEncoder is configured with handle_unknown=\"ignore\" (unknown categories are safely ignored).')\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "predict_batch",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main: predict_batch\n",
        "\n",
        "def predict_batch(\n",
        "    input_csv_path: str | Path,\n",
        "    output_csv_path: str | Path = 'predicciones_calculadora.csv',\n",
        "    threshold: float = 0.5,\n",
        "    margin: float = 0.10,\n",
        "    missing_threshold: int = 3,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Predict for a batch CSV (one row per patient).\"\"\"\n",
        "\n",
        "    input_csv_path = Path(input_csv_path)\n",
        "    if not input_csv_path.exists():\n",
        "        raise FileNotFoundError(f'Input CSV not found: {input_csv_path}')\n",
        "\n",
        "    df_in = pd.read_csv(input_csv_path)\n",
        "    if 'Unnamed: 0' in df_in.columns and df_in['Unnamed: 0'].is_unique:\n",
        "        df_in = df_in.set_index('Unnamed: 0')\n",
        "\n",
        "    X = align_dataframe(df_in)\n",
        "\n",
        "    # Profile (includes p_cluster1 + low_confidence_cluster)\n",
        "    profile_df = assign_cluster_and_subcluster(X, threshold=threshold, margin=margin)\n",
        "\n",
        "    # Survival\n",
        "    surv_df = rsf_predict_survival_at_horizons(X)\n",
        "\n",
        "    # Warnings\n",
        "    many_missing = compute_many_missing_flag(X, missing_threshold=missing_threshold)\n",
        "    unknown_cat = compute_unknown_category_flag(X)\n",
        "\n",
        "    # Risk groups\n",
        "    risk_groups = surv_df['risk_3y'].apply(lambda v: risk_group_from_risk_3y(float(v)))\n",
        "\n",
        "    out = pd.concat([profile_df, surv_df], axis=1)\n",
        "    out['risk_group'] = risk_groups\n",
        "    out['many_missing_flag'] = many_missing.astype(bool)\n",
        "    out['unknown_category_flag'] = unknown_cat.astype(bool)\n",
        "\n",
        "    # Persist\n",
        "    output_csv_path = Path(output_csv_path)\n",
        "    out.to_csv(output_csv_path, index=True)\n",
        "    print('Saved:', output_csv_path)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "demo",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running demo using: cluster/df_con_subclusters.csv\n",
            "\n",
            "--- Demo patient index: 125 ---\n",
            "\n",
            "=== Patient prediction ===\n",
            "cluster_pred: 1\n",
            "p_cluster1: 0.7505195278116013\n",
            "low_confidence_cluster: False\n",
            "subcluster_pred: 11\n",
            "p_subcluster11: 0.9936728944993931\n",
            "threshold_used: 0.5\n",
            "S(1y)=0.759 | S(3y)=0.363 | S(5y)=0.240\n",
            "risk_3y=0.637 | risk_group=alto\n",
            "Warnings: none\n",
            "\n",
            "--- Demo patient index: 51 ---\n",
            "\n",
            "=== Patient prediction ===\n",
            "cluster_pred: 1\n",
            "p_cluster1: 0.6909448148222535\n",
            "low_confidence_cluster: False\n",
            "subcluster_pred: 12\n",
            "p_subcluster11: 0.024528676267898897\n",
            "threshold_used: 0.5\n",
            "S(1y)=0.976 | S(3y)=0.888 | S(5y)=0.859\n",
            "risk_3y=0.112 | risk_group=medio\n",
            "Warnings: none\n",
            "\n",
            "Batch demo: saving predicciones_calculadora.csv\n",
            "Saved: predicciones_calculadora.csv\n",
            "Preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster_pred</th>\n",
              "      <th>p_cluster1</th>\n",
              "      <th>low_confidence_cluster</th>\n",
              "      <th>subcluster_pred</th>\n",
              "      <th>p_subcluster11</th>\n",
              "      <th>threshold_used</th>\n",
              "      <th>low_confidence_flag</th>\n",
              "      <th>S_1y</th>\n",
              "      <th>S_3y</th>\n",
              "      <th>S_5y</th>\n",
              "      <th>risk_3y</th>\n",
              "      <th>risk_group</th>\n",
              "      <th>many_missing_flag</th>\n",
              "      <th>unknown_category_flag</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.052351</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.837742</td>\n",
              "      <td>0.591839</td>\n",
              "      <td>0.403103</td>\n",
              "      <td>0.408161</td>\n",
              "      <td>alto</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.091775</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.932035</td>\n",
              "      <td>0.793764</td>\n",
              "      <td>0.697461</td>\n",
              "      <td>0.206236</td>\n",
              "      <td>medio</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.996905</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>0.042484</td>\n",
              "      <td>0.5</td>\n",
              "      <td>False</td>\n",
              "      <td>0.849486</td>\n",
              "      <td>0.557211</td>\n",
              "      <td>0.433050</td>\n",
              "      <td>0.442789</td>\n",
              "      <td>alto</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0.013298</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.975269</td>\n",
              "      <td>0.878956</td>\n",
              "      <td>0.854979</td>\n",
              "      <td>0.121044</td>\n",
              "      <td>medio</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.990009</td>\n",
              "      <td>0.959361</td>\n",
              "      <td>0.940761</td>\n",
              "      <td>0.040639</td>\n",
              "      <td>bajo</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            cluster_pred  p_cluster1  low_confidence_cluster subcluster_pred  \\\n",
              "Unnamed: 0                                                                     \n",
              "0                      2    0.052351                   False            None   \n",
              "1                      2    0.091775                   False            None   \n",
              "2                      1    0.996905                   False              12   \n",
              "3                      2    0.013298                   False            None   \n",
              "4                      2    0.010296                   False            None   \n",
              "\n",
              "            p_subcluster11  threshold_used  low_confidence_flag      S_1y  \\\n",
              "Unnamed: 0                                                                  \n",
              "0                      NaN             NaN                False  0.837742   \n",
              "1                      NaN             NaN                False  0.932035   \n",
              "2                 0.042484             0.5                False  0.849486   \n",
              "3                      NaN             NaN                False  0.975269   \n",
              "4                      NaN             NaN                False  0.990009   \n",
              "\n",
              "                S_3y      S_5y   risk_3y risk_group  many_missing_flag  \\\n",
              "Unnamed: 0                                                               \n",
              "0           0.591839  0.403103  0.408161       alto              False   \n",
              "1           0.793764  0.697461  0.206236      medio              False   \n",
              "2           0.557211  0.433050  0.442789       alto              False   \n",
              "3           0.878956  0.854979  0.121044      medio              False   \n",
              "4           0.959361  0.940761  0.040639       bajo              False   \n",
              "\n",
              "            unknown_category_flag  \n",
              "Unnamed: 0                         \n",
              "0                           False  \n",
              "1                           False  \n",
              "2                           False  \n",
              "3                           False  \n",
              "4                           False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Demo\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "demo_candidates = [\n",
        "    Path('cluster/df_con_subclusters.csv'),\n",
        "    Path('df_con_subclusters.csv'),\n",
        "    Path('cluster/df_con_clusters.csv'),\n",
        "    Path('df_con_clusters.csv'),\n",
        "]\n",
        "\n",
        "demo_path = next((p for p in demo_candidates if p.exists()), None)\n",
        "\n",
        "if demo_path is None:\n",
        "    print('No demo CSV found. To demo, provide df_con_subclusters.csv or df_con_clusters.csv.')\n",
        "else:\n",
        "    print('Running demo using:', demo_path)\n",
        "    df_demo = pd.read_csv(demo_path)\n",
        "    if 'Unnamed: 0' in df_demo.columns and df_demo['Unnamed: 0'].is_unique:\n",
        "        df_demo = df_demo.set_index('Unnamed: 0')\n",
        "\n",
        "    # Take 2 real rows\n",
        "    df_demo_2 = df_demo.sample(n=min(2, df_demo.shape[0]), random_state=RANDOM_STATE)\n",
        "\n",
        "    for idx, row in df_demo_2.iterrows():\n",
        "        patient_dict = row.to_dict()\n",
        "        print('\\n--- Demo patient index:', idx, '---')\n",
        "        _ = predict_one(patient_dict)\n",
        "\n",
        "    print('\\nBatch demo: saving predicciones_calculadora.csv')\n",
        "    pred_batch = predict_batch(demo_path, output_csv_path='predicciones_calculadora.csv')\n",
        "    print('Preview:')\n",
        "    display(pred_batch.head())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
