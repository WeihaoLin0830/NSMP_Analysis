{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b51e15a5",
      "metadata": {},
      "source": [
        "# Subclustering within parent clusters + prognostic validation (no leakage)\n",
        "\n",
        "This notebook starts from a prepared dataframe `df` that already contains a **parent cluster** column `cluster ∈ {1,2}`.\n",
        "\n",
        "Goal:\n",
        "- Find **subclusters** inside each parent cluster (1 and 2) using **only clinical features** (no outcome leakage).\n",
        "- Run two feature views to avoid self-deception:\n",
        "  - **View A (clinical phenotype):** numeric + categorical (excludes missingness/flags)\n",
        "  - **View B (operational/data availability):** numeric + categorical + flags/missing indicators\n",
        "- Profile subclusters clinically.\n",
        "- Validate whether subclusters separate prognosis **within each parent cluster** (Kaplan–Meier + log-rank).\n",
        "\n",
        "Critical anti-leakage rule:\n",
        "- `event` and `time_days` are **never** used as features for subclustering.\n",
        "- Outcomes are used **only** in the survival validation section.\n",
        "\n",
        "Outputs (written to `cluster/`):\n",
        "- `cluster/df_con_subclusters.csv`\n",
        "- `cluster/perfil_subclusters.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "271fb171",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & global settings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "random.seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "\n",
        "TARGET_EVENT = \"event\"\n",
        "TARGET_TIME = \"time_days\"\n",
        "PARENT_CLUSTER = \"cluster\"\n",
        "\n",
        "OUTPUT_DIR = Path(\"cluster\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.dpi\": 110,\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.alpha\": 0.25,\n",
        "})\n",
        "\n",
        "def _clean_cat_value(x):\n",
        "    \"\"\"Normalize category-like values (e.g., '2.0' -> '2') without creating NaNs.\"\"\"\n",
        "    if pd.isna(x):\n",
        "        return x\n",
        "    s = str(x)\n",
        "    if s.endswith(\".0\"):\n",
        "        head = s[:-2]\n",
        "        if head.replace(\"-\", \"\").isdigit():\n",
        "            return head\n",
        "    return s\n",
        "\n",
        "def clean_as_category(s: pd.Series) -> pd.Series:\n",
        "    return s.map(_clean_cat_value).astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a3a9039",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input path: cluster/df_con_clusters.csv\n",
            "df.shape: (147, 27)\n",
            "\n",
            "(df.dtypes)\n",
            " imc                           float64\n",
            "asa                          category\n",
            "valor_de_ca125                float64\n",
            "histo_defin                  category\n",
            "grado_histologi              category\n",
            "FIGO2023                     category\n",
            "tamano_tumoral                float64\n",
            "afectacion_linf              category\n",
            "metasta_distan               category\n",
            "AP_centinela_pelvico         category\n",
            "AP_ganPelv                   category\n",
            "AP_glanPaor                  category\n",
            "recep_est_porcent             float64\n",
            "rece_de_Ppor                  float64\n",
            "beta_cateninap               category\n",
            "event                           int64\n",
            "time_days                       int64\n",
            "edad_en_cirugia               float64\n",
            "histo_defin__from_pre            int8\n",
            "grado_histologi__from_pre        int8\n",
            "FIGO2023__from_pre               int8\n",
            "imc_miss                         int8\n",
            "tamano_tumoral_miss              int8\n",
            "valor_de_ca125_miss              int8\n",
            "recep_est_porcent_miss           int8\n",
            "rece_de_Ppor_miss                int8\n",
            "cluster                         int64\n",
            "dtype: object\n",
            "\n",
            "(df.head)\n",
            "              imc asa  valor_de_ca125 histo_defin grado_histologi FIGO2023  \\\n",
            "Unnamed: 0                                                                  \n",
            "0           39.4   2        5.648974           2               2       14   \n",
            "1           38.8   2        3.226844           2               1        1   \n",
            "2           36.3   1        3.226844           2               1       11   \n",
            "3           31.1   1        3.226844           2               1        1   \n",
            "4           40.6   1        3.226844           2               1        1   \n",
            "\n",
            "            tamano_tumoral afectacion_linf metasta_distan  \\\n",
            "Unnamed: 0                                                  \n",
            "0                 1.386294         Missing              1   \n",
            "1                 1.098612               1              0   \n",
            "2                 2.079442               1              0   \n",
            "3                 1.791759               0              0   \n",
            "4                 0.530628               0              0   \n",
            "\n",
            "           AP_centinela_pelvico  ... edad_en_cirugia histo_defin__from_pre  \\\n",
            "Unnamed: 0                       ...                                         \n",
            "0                             4  ...       68.095825                     0   \n",
            "1                             4  ...       72.802190                     0   \n",
            "2                             4  ...       78.422998                     0   \n",
            "3                             4  ...       68.438056                     0   \n",
            "4                             4  ...       58.272416                     0   \n",
            "\n",
            "            grado_histologi__from_pre  FIGO2023__from_pre imc_miss  \\\n",
            "Unnamed: 0                                                           \n",
            "0                                   0                   0        0   \n",
            "1                                   0                   0        0   \n",
            "2                                   0                   0        0   \n",
            "3                                   0                   0        0   \n",
            "4                                   0                   0        0   \n",
            "\n",
            "            tamano_tumoral_miss  valor_de_ca125_miss  recep_est_porcent_miss  \\\n",
            "Unnamed: 0                                                                     \n",
            "0                             1                    0                       0   \n",
            "1                             0                    1                       1   \n",
            "2                             0                    1                       0   \n",
            "3                             0                    1                       0   \n",
            "4                             0                    1                       1   \n",
            "\n",
            "            rece_de_Ppor_miss  cluster  \n",
            "Unnamed: 0                              \n",
            "0                           0        2  \n",
            "1                           1        2  \n",
            "2                           0        1  \n",
            "3                           0        1  \n",
            "4                           1        2  \n",
            "\n",
            "[5 rows x 27 columns]\n"
          ]
        }
      ],
      "source": [
        "# 0) Load df (must already include parent cluster column)\n",
        "\n",
        "CANDIDATE_INPUTS = [\n",
        "    Path(\"cluster/df_con_clusters.csv\"),\n",
        "    Path(\"df_con_clusters.csv\"),\n",
        "    Path(\"Dades/dt_model.csv\"),\n",
        "]\n",
        "\n",
        "INPUT_PATH = next((p for p in CANDIDATE_INPUTS if p.exists()), None)\n",
        "if INPUT_PATH is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find an input CSV. Expected one of: \" + \", \".join(map(str, CANDIDATE_INPUTS))\n",
        "    )\n",
        "\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "if \"Unnamed: 0\" in df.columns and df[\"Unnamed: 0\"].is_unique:\n",
        "    df = df.set_index(\"Unnamed: 0\")\n",
        "\n",
        "# Column lists (as specified)\n",
        "num_cols = [\n",
        "    \"imc\",\n",
        "    \"valor_de_ca125\",\n",
        "    \"tamano_tumoral\",\n",
        "    \"recep_est_porcent\",\n",
        "    \"rece_de_Ppor\",\n",
        "    \"edad_en_cirugia\",\n",
        "]\n",
        "\n",
        "cat_cols = [\n",
        "    \"asa\",\n",
        "    \"histo_defin\",\n",
        "    \"grado_histologi\",\n",
        "    \"FIGO2023\",\n",
        "    \"afectacion_linf\",\n",
        "    \"metasta_distan\",\n",
        "    \"AP_centinela_pelvico\",\n",
        "    \"AP_ganPelv\",\n",
        "    \"AP_glanPaor\",\n",
        "    \"beta_cateninap\",\n",
        "]\n",
        "\n",
        "flag_cols = [\n",
        "    \"histo_defin__from_pre\",\n",
        "    \"grado_histologi__from_pre\",\n",
        "    \"FIGO2023__from_pre\",\n",
        "    \"imc_miss\",\n",
        "    \"tamano_tumoral_miss\",\n",
        "    \"valor_de_ca125_miss\",\n",
        "    \"recep_est_porcent_miss\",\n",
        "    \"rece_de_Ppor_miss\",\n",
        "]\n",
        "\n",
        "required_cols = set(num_cols + cat_cols + flag_cols + [TARGET_EVENT, TARGET_TIME, PARENT_CLUSTER])\n",
        "missing = sorted(required_cols - set(df.columns))\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Enforce dtypes\n",
        "for col in num_cols:\n",
        "    df[col] = df[col].astype(\"float64\")\n",
        "for col in cat_cols:\n",
        "    df[col] = clean_as_category(df[col])\n",
        "for col in flag_cols:\n",
        "    df[col] = df[col].astype(\"int8\")\n",
        "\n",
        "df[TARGET_EVENT] = df[TARGET_EVENT].astype(\"int64\")\n",
        "df[TARGET_TIME] = df[TARGET_TIME].astype(\"int64\")\n",
        "df[PARENT_CLUSTER] = df[PARENT_CLUSTER].astype(\"int64\")\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in [TARGET_EVENT, TARGET_TIME, PARENT_CLUSTER]]\n",
        "expected_feature_set = set(num_cols + cat_cols + flag_cols)\n",
        "if set(feature_cols) != expected_feature_set:\n",
        "    missing_f = sorted(expected_feature_set - set(feature_cols))\n",
        "    extra_f = sorted(set(feature_cols) - expected_feature_set)\n",
        "    raise ValueError(\n",
        "        \"feature_cols mismatch vs expected schema. \"\n",
        "        f\"Missing: {missing_f}; Extra: {extra_f}\"\n",
        "    )\n",
        "\n",
        "# Hard checks\n",
        "if df.isna().any().any():\n",
        "    na_cols = df.columns[df.isna().any()].tolist()\n",
        "    raise ValueError(f\"NaNs detected in columns: {na_cols}\")\n",
        "\n",
        "event_values = set(df[TARGET_EVENT].unique().tolist())\n",
        "if not event_values.issubset({0, 1}):\n",
        "    raise ValueError(f\"{TARGET_EVENT} must be binary {{0,1}}; got {sorted(event_values)}\")\n",
        "\n",
        "if (df[TARGET_TIME] < 0).any():\n",
        "    raise ValueError(f\"{TARGET_TIME} contains negative values\")\n",
        "\n",
        "clusters = set(df[PARENT_CLUSTER].unique().tolist())\n",
        "if clusters != {1, 2}:\n",
        "    raise ValueError(f\"{PARENT_CLUSTER} must have exactly values {{1,2}}; got {sorted(clusters)}\")\n",
        "\n",
        "print(\"Input path:\", INPUT_PATH)\n",
        "print(\"df.shape:\", df.shape)\n",
        "print(\"\\n(df.dtypes)\\n\", df.dtypes)\n",
        "print(\"\\n(df.head)\\n\", df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bbfabc58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent cluster sizes:\n",
            "cluster\n",
            "1    86\n",
            "2    61\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Event-rate and time_days summary by parent cluster:\n",
            "- Parent cluster 1: n=86, events=17, event-rate=0.198, time_days min/median/max=101/1107.0/2303\n",
            "- Parent cluster 2: n=61, events=20, event-rate=0.328, time_days min/median/max=0/915.0/2481\n"
          ]
        }
      ],
      "source": [
        "# 1) Mandatory audit\n",
        "\n",
        "sizes_parent = df[PARENT_CLUSTER].value_counts().sort_index()\n",
        "print(\"Parent cluster sizes:\")\n",
        "print(sizes_parent)\n",
        "\n",
        "print(\"\\nEvent-rate and time_days summary by parent cluster:\")\n",
        "for p in [1, 2]:\n",
        "    sub = df[df[PARENT_CLUSTER] == p]\n",
        "    er = float(sub[TARGET_EVENT].mean())\n",
        "    tmin = int(sub[TARGET_TIME].min())\n",
        "    tmed = float(sub[TARGET_TIME].median())\n",
        "    tmax = int(sub[TARGET_TIME].max())\n",
        "    nevents = int(sub[TARGET_EVENT].sum())\n",
        "    print(f\"- Parent cluster {p}: n={sub.shape[0]}, events={nevents}, event-rate={er:.3f}, time_days min/median/max={tmin}/{tmed:.1f}/{tmax}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d38dd6",
      "metadata": {},
      "source": [
        "## 2) Subclustering approach: two views\n",
        "\n",
        "For each parent cluster (1 and 2) we run two subclustering configurations:\n",
        "\n",
        "- **View A (clinical phenotype):** `num_cols + cat_cols` (excludes `*_miss` and `__from_pre`)\n",
        "- **View B (operational/data availability):** `num_cols + cat_cols + flag_cols`\n",
        "\n",
        "Distance + algorithm:\n",
        "- Mixed data distance: **Gower** (package if installed, else manual implementation)\n",
        "- Main algorithm: **hierarchical clustering** with **average linkage** on precomputed distances\n",
        "- Optional alternative: **k-medoids (PAM)** on precomputed distances (only if available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4cbbcc73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMedoids available: True\n"
          ]
        }
      ],
      "source": [
        "# 3) Utilities: Gower distance, clustering, silhouette, bootstrap stability\n",
        "\n",
        "def gower_distance_matrix_manual(\n",
        "    X: pd.DataFrame,\n",
        "    num_cols: list[str],\n",
        "    cat_cols: list[str],\n",
        "    flag_cols: list[str],\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Manual Gower distance for mixed data.\n",
        "\n",
        "    IMPORTANT: numeric ranges are computed within X (here: within the parent cluster).\n",
        "    \"\"\"\n",
        "    n = X.shape[0]\n",
        "    p = len(num_cols) + len(cat_cols) + len(flag_cols)\n",
        "    if p == 0:\n",
        "        raise ValueError(\"No features provided for Gower distance\")\n",
        "\n",
        "    D = np.zeros((n, n), dtype=np.float64)\n",
        "\n",
        "    # Numeric part: min-max normalization within the parent cluster\n",
        "    if num_cols:\n",
        "        num = X[num_cols].to_numpy(dtype=np.float64)\n",
        "        mins = num.min(axis=0)\n",
        "        maxs = num.max(axis=0)\n",
        "        ranges = maxs - mins\n",
        "        ranges[ranges == 0.0] = 1.0\n",
        "        num_norm = (num - mins) / ranges\n",
        "        for j in range(num_norm.shape[1]):\n",
        "            col = num_norm[:, j]\n",
        "            D += np.abs(col[:, None] - col[None, :])\n",
        "\n",
        "    # Categorical + flags part: 0 if equal else 1\n",
        "    for col in (cat_cols + flag_cols):\n",
        "        codes = pd.Categorical(X[col]).codes\n",
        "        D += (codes[:, None] != codes[None, :]).astype(np.float64)\n",
        "\n",
        "    D /= float(p)\n",
        "    np.fill_diagonal(D, 0.0)\n",
        "    D = (D + D.T) / 2.0\n",
        "    return D\n",
        "\n",
        "def compute_gower_distance_matrix(\n",
        "    X: pd.DataFrame,\n",
        "    num_cols: list[str],\n",
        "    cat_cols: list[str],\n",
        "    flag_cols: list[str],\n",
        ") -> tuple[np.ndarray, str]:\n",
        "    \"\"\"Compute Gower distance matrix (NxN) with a package fallback.\"\"\"\n",
        "    try:\n",
        "        import gower  # type: ignore\n",
        "        D = np.asarray(gower.gower_matrix(X), dtype=np.float64)\n",
        "        backend = \"gower (package)\"\n",
        "    except ImportError:\n",
        "        D = gower_distance_matrix_manual(X, num_cols=num_cols, cat_cols=cat_cols, flag_cols=flag_cols)\n",
        "        backend = \"manual\"\n",
        "    return D, backend\n",
        "\n",
        "def hierarchical_linkage_from_distance(D: np.ndarray, method: str = \"average\"):\n",
        "    return linkage(squareform(D, checks=False), method=method)\n",
        "\n",
        "def hierarchical_cut(Z, K: int) -> np.ndarray:\n",
        "    return fcluster(Z, t=K, criterion=\"maxclust\").astype(int)\n",
        "\n",
        "def safe_silhouette_precomputed(D: np.ndarray, labels: np.ndarray) -> float:\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        return float(\"nan\")\n",
        "    counts = pd.Series(labels).value_counts()\n",
        "    if int(counts.min()) < 2:\n",
        "        return float(\"nan\")\n",
        "    try:\n",
        "        return float(silhouette_score(D, labels, metric=\"precomputed\"))\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def silhouette_pca_onehot(\n",
        "    X: pd.DataFrame,\n",
        "    labels: np.ndarray,\n",
        "    num_cols: list[str],\n",
        "    cat_cols: list[str],\n",
        "    flag_cols: list[str],\n",
        "    random_state: int = 42,\n",
        ") -> float:\n",
        "    \"\"\"Approximate silhouette using a PCA embedding of one-hot features.\n",
        "\n",
        "    Used only if silhouette on precomputed distances is not available.\n",
        "    \"\"\"\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        return float(\"nan\")\n",
        "    counts = pd.Series(labels).value_counts()\n",
        "    if int(counts.min()) < 2:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    X_num = StandardScaler().fit_transform(X[num_cols]) if num_cols else np.empty((X.shape[0], 0))\n",
        "    X_cat = pd.get_dummies(X[cat_cols].astype(str), drop_first=False) if cat_cols else pd.DataFrame(index=X.index)\n",
        "    X_flag = X[flag_cols].astype(float).to_numpy() if flag_cols else np.empty((X.shape[0], 0))\n",
        "\n",
        "    X_mat = np.hstack([X_num, X_cat.to_numpy(), X_flag])\n",
        "    if X_mat.shape[1] < 2 or X_mat.shape[0] < 3:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    n_components = int(min(10, X_mat.shape[1], X_mat.shape[0] - 1))\n",
        "    if n_components < 2:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    X_emb = PCA(n_components=n_components, random_state=random_state).fit_transform(X_mat)\n",
        "    try:\n",
        "        return float(silhouette_score(X_emb, labels, metric=\"euclidean\"))\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def bootstrap_stability_ari(\n",
        "    D: np.ndarray,\n",
        "    labels_ref: np.ndarray,\n",
        "    K: int,\n",
        "    linkage_method: str,\n",
        "    B: int,\n",
        "    rng: np.random.Generator,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"Bootstrap stability (ARI) using resampling with replacement.\n",
        "\n",
        "    We follow an \"intersection\" approach:\n",
        "    - draw bootstrap indices with replacement\n",
        "    - keep unique indices (the intersection set)\n",
        "    - recluster on the induced distance submatrix\n",
        "    - compare with the original clustering restricted to the same unique indices\n",
        "    \"\"\"\n",
        "    n = D.shape[0]\n",
        "    aris = []\n",
        "    for _ in range(B):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        uniq = np.unique(idx)\n",
        "        if uniq.size < max(2, K):\n",
        "            continue\n",
        "        D_b = D[np.ix_(uniq, uniq)]\n",
        "        Z_b = hierarchical_linkage_from_distance(D_b, method=linkage_method)\n",
        "        labels_b = hierarchical_cut(Z_b, K)\n",
        "        aris.append(adjusted_rand_score(labels_ref[uniq], labels_b))\n",
        "    if len(aris) == 0:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    return float(np.mean(aris)), float(np.std(aris))\n",
        "\n",
        "# Optional k-medoids (PAM) if available\n",
        "try:\n",
        "    from sklearn_extra.cluster import KMedoids  # type: ignore\n",
        "    KMEDOIDS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KMEDOIDS_AVAILABLE = False\n",
        "\n",
        "print(\"KMedoids available:\", KMEDOIDS_AVAILABLE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8959d381",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "Parent cluster 1: n=86 | min_cluster_size threshold=10\n",
            "\n",
            "--- View A (clinical phenotype; excludes flags/missingness) ---\n",
            "   K              status  min_cluster_size              cluster_sizes  \\\n",
            "0  2  discarded_min_size                 1              {1: 85, 2: 1}   \n",
            "1  3  discarded_min_size                 1        {1: 84, 2: 1, 3: 1}   \n",
            "2  4  discarded_min_size                 1  {1: 83, 2: 1, 3: 1, 4: 1}   \n",
            "\n",
            "   silhouette  silhouette_method  ARI_mean   ARI_std  \n",
            "0         NaN  pca_onehot_approx  0.569870  0.475046  \n",
            "1         NaN  pca_onehot_approx  0.554537  0.324414  \n",
            "2         NaN  pca_onehot_approx  0.507781  0.288841  \n",
            "No K in [2, 3, 4] passes min_cluster_size for parent=1, view=A.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 83\u001b[0m\n\u001b[1;32m     73\u001b[0m         sil \u001b[38;5;241m=\u001b[39m silhouette_pca_onehot(\n\u001b[1;32m     74\u001b[0m             X_view,\n\u001b[1;32m     75\u001b[0m             labels_k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m             random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m         sil_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_onehot_approx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m     ari_mean, ari_std \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap_stability_ari\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinkage_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLINKAGE_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB_BOOTSTRAP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m: parent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m         }\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[4], line 137\u001b[0m, in \u001b[0;36mbootstrap_stability_ari\u001b[0;34m(D, labels_ref, K, linkage_method, B, rng)\u001b[0m\n\u001b[1;32m    135\u001b[0m     Z_b \u001b[38;5;241m=\u001b[39m hierarchical_linkage_from_distance(D_b, method\u001b[38;5;241m=\u001b[39mlinkage_method)\n\u001b[1;32m    136\u001b[0m     labels_b \u001b[38;5;241m=\u001b[39m hierarchical_cut(Z_b, K)\n\u001b[0;32m--> 137\u001b[0m     aris\u001b[38;5;241m.\u001b[39mappend(\u001b[43madjusted_rand_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[43muniq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_b\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(aris) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/metrics/cluster/_supervised.py:443\u001b[0m, in \u001b[0;36madjusted_rand_score\u001b[0;34m(labels_true, labels_pred)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    343\u001b[0m     {\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madjusted_rand_score\u001b[39m(labels_true, labels_pred):\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Rand index adjusted for chance.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    The Rand Index computes a similarity measure between two clusterings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    for a more detailed example.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     (tn, fp), (fn, tp) \u001b[38;5;241m=\u001b[39m \u001b[43mpair_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# convert to Python integer types, to avoid overflow or underflow\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(tn), \u001b[38;5;28mint\u001b[39m(fp), \u001b[38;5;28mint\u001b[39m(fn), \u001b[38;5;28mint\u001b[39m(tp)\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/metrics/cluster/_supervised.py:248\u001b[0m, in \u001b[0;36mpair_confusion_matrix\u001b[0;34m(labels_true, labels_pred)\u001b[0m\n\u001b[1;32m    245\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint64(labels_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Computation using the contingency data\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m contingency \u001b[38;5;241m=\u001b[39m \u001b[43mcontingency_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m n_c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(contingency\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    252\u001b[0m n_k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(contingency\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/sklearn/metrics/cluster/_supervised.py:151\u001b[0m, in \u001b[0;36mcontingency_matrix\u001b[0;34m(labels_true, labels_pred, eps, sparse, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sparse:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when sparse=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m classes, class_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m clusters, cluster_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels_pred, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    153\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/numpy/lib/_arraysetops_impl.py:286\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    284\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/numpy/lib/_arraysetops_impl.py:375\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis)\u001b[0m\n\u001b[1;32m    373\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (perm[mask],)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_inverse:\n\u001b[0;32m--> 375\u001b[0m     imask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    376\u001b[0m     inv_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(mask\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m    377\u001b[0m     inv_idx[perm] \u001b[38;5;241m=\u001b[39m imask\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:2955\u001b[0m, in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_cumsum_dispatcher)\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcumsum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;124;03m    Return the cumulative sum of the elements along a given axis.\u001b[39;00m\n\u001b[1;32m   2883\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \n\u001b[1;32m   2954\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcumsum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/proyecto-ia/bitsxmt25/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 4) Subclustering within each parent cluster (two views)\n",
        "\n",
        "LINKAGE_METHOD = \"average\"\n",
        "K_CANDIDATES = [2, 3, 4]\n",
        "B_BOOTSTRAP = 200\n",
        "\n",
        "view_defs = {\n",
        "    \"A\": {\n",
        "        \"label\": \"View A (clinical phenotype; excludes flags/missingness)\",\n",
        "        \"num_cols\": num_cols,\n",
        "        \"cat_cols\": cat_cols,\n",
        "        \"flag_cols\": [],\n",
        "    },\n",
        "    \"B\": {\n",
        "        \"label\": \"View B (operational/data availability; includes flags/missingness)\",\n",
        "        \"num_cols\": num_cols,\n",
        "        \"cat_cols\": cat_cols,\n",
        "        \"flag_cols\": flag_cols,\n",
        "    },\n",
        "}\n",
        "\n",
        "df_sub = df.copy()\n",
        "df_sub[\"subcluster_A\"] = pd.Series(pd.NA, index=df_sub.index, dtype=\"Int64\")\n",
        "df_sub[\"subcluster_B\"] = pd.Series(pd.NA, index=df_sub.index, dtype=\"Int64\")\n",
        "df_sub[\"subcluster_final\"] = pd.Series(0, index=df_sub.index, dtype=\"Int64\")\n",
        "\n",
        "subclustering_results = {}\n",
        "\n",
        "def min_cluster_size_threshold(n_parent: int) -> int:\n",
        "    # As specified: default >=10; if parent is small, relax to >= max(8, 0.15*n_parent)\n",
        "    if n_parent < 60:\n",
        "        return int(max(8, np.ceil(0.15 * n_parent)))\n",
        "    return 10\n",
        "\n",
        "for parent in [1, 2]:\n",
        "    df_p = df_sub[df_sub[PARENT_CLUSTER] == parent].copy()\n",
        "    n_parent = int(df_p.shape[0])\n",
        "    min_size = min_cluster_size_threshold(n_parent)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\"Parent cluster {parent}: n={n_parent} | min_cluster_size threshold={min_size}\")\n",
        "\n",
        "    subclustering_results[parent] = {\"n_parent\": n_parent, \"min_cluster_size\": min_size, \"views\": {}}\n",
        "\n",
        "    for view_key, view in view_defs.items():\n",
        "        cols_view = view[\"num_cols\"] + view[\"cat_cols\"] + view[\"flag_cols\"]\n",
        "        X_view = df_p[cols_view].copy()\n",
        "\n",
        "        D, gower_backend = compute_gower_distance_matrix(\n",
        "            X_view,\n",
        "            num_cols=view[\"num_cols\"],\n",
        "            cat_cols=view[\"cat_cols\"],\n",
        "            flag_cols=view[\"flag_cols\"],\n",
        "        )\n",
        "        Z = hierarchical_linkage_from_distance(D, method=LINKAGE_METHOD)\n",
        "\n",
        "        rows = []\n",
        "        labels_by_K = {}\n",
        "\n",
        "        for K in K_CANDIDATES:\n",
        "            labels_k = hierarchical_cut(Z, K)\n",
        "            labels_by_K[K] = labels_k\n",
        "\n",
        "            sizes = pd.Series(labels_k).value_counts().sort_index()\n",
        "            min_k = int(sizes.min())\n",
        "            max_k = int(sizes.max())\n",
        "            status = \"ok\" if min_k >= min_size else \"discarded_min_size\"\n",
        "\n",
        "            sil = safe_silhouette_precomputed(D, labels_k)\n",
        "            sil_method = \"precomputed\"\n",
        "            if np.isnan(sil):\n",
        "                # Fallback only for reporting (explicitly approximate)\n",
        "                sil = silhouette_pca_onehot(\n",
        "                    X_view,\n",
        "                    labels_k,\n",
        "                    num_cols=view[\"num_cols\"],\n",
        "                    cat_cols=view[\"cat_cols\"],\n",
        "                    flag_cols=view[\"flag_cols\"],\n",
        "                    random_state=RANDOM_STATE,\n",
        "                )\n",
        "                sil_method = \"pca_onehot_approx\"\n",
        "\n",
        "            ari_mean, ari_std = bootstrap_stability_ari(\n",
        "                D,\n",
        "                labels_ref=labels_k,\n",
        "                K=K,\n",
        "                linkage_method=LINKAGE_METHOD,\n",
        "                B=B_BOOTSTRAP,\n",
        "                rng=rng,\n",
        "            )\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"parent_cluster\": parent,\n",
        "                    \"view\": view_key,\n",
        "                    \"K\": K,\n",
        "                    \"status\": status,\n",
        "                    \"silhouette\": float(sil),\n",
        "                    \"silhouette_method\": sil_method,\n",
        "                    \"min_cluster_size\": min_k,\n",
        "                    \"max_cluster_size\": max_k,\n",
        "                    \"balance_ratio_min_over_max\": float(min_k / max_k) if max_k > 0 else float(\"nan\"),\n",
        "                    \"cluster_sizes\": dict(sizes),\n",
        "                    \"ARI_mean\": float(ari_mean),\n",
        "                    \"ARI_std\": float(ari_std),\n",
        "                    \"gower_backend\": gower_backend,\n",
        "                    \"linkage_method\": LINKAGE_METHOD,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        metrics_df = pd.DataFrame(rows).sort_values(\"K\").reset_index(drop=True)\n",
        "        print(\"\\n---\", view[\"label\"], \"---\")\n",
        "        print(metrics_df[[\"K\", \"status\", \"min_cluster_size\", \"cluster_sizes\", \"silhouette\", \"silhouette_method\", \"ARI_mean\", \"ARI_std\"]])\n",
        "\n",
        "        # Selection rule: prioritize ARI, then balance, then silhouette\n",
        "        candidates = metrics_df[metrics_df[\"status\"] == \"ok\"].copy()\n",
        "        if candidates.empty:\n",
        "            selected = None\n",
        "            print(f\"No K in {K_CANDIDATES} passes min_cluster_size for parent={parent}, view={view_key}.\")\n",
        "        else:\n",
        "            candidates = candidates.sort_values(\n",
        "                [\"ARI_mean\", \"balance_ratio_min_over_max\", \"silhouette\"],\n",
        "                ascending=[False, False, False],\n",
        "            )\n",
        "            selected = candidates.iloc[0].to_dict()\n",
        "            print(\n",
        "                f\"Selected for parent={parent}, view={view_key}: K={int(selected['K'])} \"\n",
        "                f\"(ARI={selected['ARI_mean']:.3f}±{selected['ARI_std']:.3f}, silhouette={selected['silhouette']:.3f}, sizes={selected['cluster_sizes']})\"\n",
        "            )\n",
        "\n",
        "        # Optional k-medoids comparison for the selected K\n",
        "        kmedoids_labels = None\n",
        "        if selected is not None and KMEDOIDS_AVAILABLE:\n",
        "            K_sel = int(selected[\"K\"])\n",
        "            km = KMedoids(\n",
        "                n_clusters=K_sel,\n",
        "                metric=\"precomputed\",\n",
        "                method=\"pam\",\n",
        "                init=\"heuristic\",\n",
        "                random_state=RANDOM_STATE,\n",
        "            )\n",
        "            km.fit(D)\n",
        "            kmedoids_labels = (km.labels_.astype(int) + 1)  # 1..K\n",
        "            ari_h_vs_km = adjusted_rand_score(labels_by_K[K_sel], kmedoids_labels)\n",
        "            print(f\"K-medoids available: ARI(hierarchical vs k-medoids)={ari_h_vs_km:.3f}\")\n",
        "\n",
        "        # Store per-view results\n",
        "        subclustering_results[parent][\"views\"][view_key] = {\n",
        "            \"view_label\": view[\"label\"],\n",
        "            \"cols_view\": cols_view,\n",
        "            \"gower_backend\": gower_backend,\n",
        "            \"metrics_by_K\": metrics_df,\n",
        "            \"selected\": selected,\n",
        "            \"labels_by_K\": labels_by_K,\n",
        "            \"kmedoids_labels_selected\": kmedoids_labels,\n",
        "        }\n",
        "\n",
        "        # Assign labels to df_sub (global code = parent*10 + local_label)\n",
        "        if selected is not None:\n",
        "            K_sel = int(selected[\"K\"])\n",
        "            local_labels = labels_by_K[K_sel]\n",
        "            global_codes = (parent * 10 + local_labels).astype(int)\n",
        "\n",
        "            target_col = \"subcluster_A\" if view_key == \"A\" else \"subcluster_B\"\n",
        "            df_sub.loc[df_p.index, target_col] = pd.Series(global_codes, index=df_p.index, dtype=\"Int64\")\n",
        "\n",
        "# Define recommended subcluster_final for each parent cluster\n",
        "view_used_final = {}\n",
        "for parent in [1, 2]:\n",
        "    idx_parent = df_sub[PARENT_CLUSTER] == parent\n",
        "    sel_A = subclustering_results[parent][\"views\"][\"A\"][\"selected\"]\n",
        "    sel_B = subclustering_results[parent][\"views\"][\"B\"][\"selected\"]\n",
        "\n",
        "    if sel_A is not None:\n",
        "        df_sub.loc[idx_parent, \"subcluster_final\"] = df_sub.loc[idx_parent, \"subcluster_A\"].astype(\"Int64\")\n",
        "        view_used_final[parent] = \"A\"\n",
        "    elif sel_B is not None:\n",
        "        df_sub.loc[idx_parent, \"subcluster_final\"] = df_sub.loc[idx_parent, \"subcluster_B\"].astype(\"Int64\")\n",
        "        view_used_final[parent] = \"B\"\n",
        "    else:\n",
        "        df_sub.loc[idx_parent, \"subcluster_final\"] = 0\n",
        "        view_used_final[parent] = None\n",
        "\n",
        "print(\"\\nSubcluster counts by parent cluster (using subcluster_final):\")\n",
        "print(df_sub.groupby([PARENT_CLUSTER, \"subcluster_final\"]).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7d3fad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported: cluster/df_con_subclusters.csv\n"
          ]
        }
      ],
      "source": [
        "# 5) Export df with subclusters\n",
        "\n",
        "out_path_df = OUTPUT_DIR / \"df_con_subclusters.csv\"\n",
        "df_sub.to_csv(out_path_df, index=True)\n",
        "print(\"Exported:\", out_path_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7724d76a",
      "metadata": {},
      "source": [
        "## 6) Profiling subclusters (per parent cluster)\n",
        "\n",
        "For each parent cluster and each `subcluster_final`:\n",
        "- Numeric: median (IQR)\n",
        "- Categorical: % per category\n",
        "- Flags/missing indicators: % of 1s\n",
        "\n",
        "We also compute differential variables across subclusters:\n",
        "- Numeric: Kruskal–Wallis\n",
        "- Categorical/flags: Chi² (or Fisher exact when 2×2 and expected counts are low)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42040f5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported: cluster/perfil_subclusters.csv\n",
            "\n",
            "Perfil_subclusters preview:\n",
            "   parent_cluster  subcluster_final         type           variable    level  \\\n",
            "0               1                 0      numeric                imc            \n",
            "1               1                 0      numeric     valor_de_ca125            \n",
            "2               1                 0      numeric     tamano_tumoral            \n",
            "3               1                 0      numeric  recep_est_porcent            \n",
            "4               1                 0      numeric       rece_de_Ppor            \n",
            "5               1                 0      numeric    edad_en_cirugia            \n",
            "6               1                 0  categorical                asa        1   \n",
            "7               1                 0  categorical                asa        2   \n",
            "8               1                 0  categorical                asa        0   \n",
            "9               1                 0  categorical                asa  Missing   \n",
            "\n",
            "    n     median         q1         q3  count        pct  pct1  \\\n",
            "0  86  30.950000  25.525000  35.475000    NaN        NaN   NaN   \n",
            "1  86   3.226844   3.226844   3.226844    NaN        NaN   NaN   \n",
            "2  86   1.266848   1.038392   1.609438    NaN        NaN   NaN   \n",
            "3  86   0.900000   0.700000   0.900000    NaN        NaN   NaN   \n",
            "4  86   0.800000   0.600000   0.900000    NaN        NaN   NaN   \n",
            "5  86  62.713210  54.126626  69.817248    NaN        NaN   NaN   \n",
            "6  86        NaN        NaN        NaN   61.0  70.930233   NaN   \n",
            "7  86        NaN        NaN        NaN   17.0  19.767442   NaN   \n",
            "8  86        NaN        NaN        NaN    7.0   8.139535   NaN   \n",
            "9  86        NaN        NaN        NaN    1.0   1.162791   NaN   \n",
            "\n",
            "                summary  \n",
            "0  30.95 [25.52, 35.48]  \n",
            "1     3.23 [3.23, 3.23]  \n",
            "2     1.27 [1.04, 1.61]  \n",
            "3     0.90 [0.70, 0.90]  \n",
            "4     0.80 [0.60, 0.90]  \n",
            "5  62.71 [54.13, 69.82]  \n",
            "6                        \n",
            "7                        \n",
            "8                        \n",
            "9                        \n",
            "\n",
            "Narrative profiles:\n",
            "\n",
            "Parent cluster 1:\n",
            "- subcluster_final=0: Perfil descriptivo (no causal): imc: median 31.0 vs parent 31.0 (Δ=+0.0); valor_de_ca125: median 3.2 vs parent 3.2 (Δ=+0.0); tamano_tumoral: median 1.3 vs parent 1.3 (Δ=+0.0)\n",
            "\n",
            "Parent cluster 2:\n",
            "- subcluster_final=0: Perfil descriptivo (no causal): imc: median 29.4 vs parent 29.4 (Δ=+0.0); valor_de_ca125: median 3.2 vs parent 3.2 (Δ=+0.0); tamano_tumoral: median 1.4 vs parent 1.4 (Δ=+0.0)\n"
          ]
        }
      ],
      "source": [
        "def fmt_median_iqr(x: pd.Series) -> tuple[float, float, float, str]:\n",
        "    q1 = float(x.quantile(0.25))\n",
        "    q3 = float(x.quantile(0.75))\n",
        "    med = float(x.median())\n",
        "    return med, q1, q3, f\"{med:.2f} [{q1:.2f}, {q3:.2f}]\"\n",
        "\n",
        "def epsilon_squared_from_kruskal(H: float, k: int, n: int) -> float:\n",
        "    if n <= k:\n",
        "        return float(\"nan\")\n",
        "    return max(0.0, float((H - k + 1) / (n - k)))\n",
        "\n",
        "def cramers_v_from_chi2(chi2: float, n: int, r: int, c: int) -> float:\n",
        "    denom = n * (min(r - 1, c - 1))\n",
        "    if denom <= 0:\n",
        "        return float(\"nan\")\n",
        "    return float(np.sqrt(chi2 / denom))\n",
        "\n",
        "profile_rows = []\n",
        "diff_rows = []\n",
        "narratives = []\n",
        "\n",
        "for parent in [1, 2]:\n",
        "    df_p = df_sub[df_sub[PARENT_CLUSTER] == parent].copy()\n",
        "    groups = sorted(df_p[\"subcluster_final\"].unique().tolist())\n",
        "\n",
        "    # Profiling tables\n",
        "    for sc, sub in df_p.groupby(\"subcluster_final\"):\n",
        "        n = int(sub.shape[0])\n",
        "\n",
        "        for col in num_cols:\n",
        "            med, q1, q3, summary = fmt_median_iqr(sub[col])\n",
        "            profile_rows.append(\n",
        "                {\n",
        "                    \"parent_cluster\": parent,\n",
        "                    \"subcluster_final\": int(sc),\n",
        "                    \"type\": \"numeric\",\n",
        "                    \"variable\": col,\n",
        "                    \"level\": \"\",\n",
        "                    \"n\": n,\n",
        "                    \"median\": med,\n",
        "                    \"q1\": q1,\n",
        "                    \"q3\": q3,\n",
        "                    \"count\": np.nan,\n",
        "                    \"pct\": np.nan,\n",
        "                    \"pct1\": np.nan,\n",
        "                    \"summary\": summary,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        for col in cat_cols:\n",
        "            vc = sub[col].value_counts(dropna=False)\n",
        "            for lvl, cnt in vc.items():\n",
        "                pct = float(cnt / n * 100.0)\n",
        "                profile_rows.append(\n",
        "                    {\n",
        "                        \"parent_cluster\": parent,\n",
        "                        \"subcluster_final\": int(sc),\n",
        "                        \"type\": \"categorical\",\n",
        "                        \"variable\": col,\n",
        "                        \"level\": str(lvl),\n",
        "                        \"n\": n,\n",
        "                        \"median\": np.nan,\n",
        "                        \"q1\": np.nan,\n",
        "                        \"q3\": np.nan,\n",
        "                        \"count\": int(cnt),\n",
        "                        \"pct\": pct,\n",
        "                        \"pct1\": np.nan,\n",
        "                        \"summary\": \"\",\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        for col in flag_cols:\n",
        "            pct1 = float(sub[col].mean() * 100.0)\n",
        "            profile_rows.append(\n",
        "                {\n",
        "                    \"parent_cluster\": parent,\n",
        "                    \"subcluster_final\": int(sc),\n",
        "                    \"type\": \"flag\",\n",
        "                    \"variable\": col,\n",
        "                    \"level\": \"\",\n",
        "                    \"n\": n,\n",
        "                    \"median\": np.nan,\n",
        "                    \"q1\": np.nan,\n",
        "                    \"q3\": np.nan,\n",
        "                    \"count\": np.nan,\n",
        "                    \"pct\": np.nan,\n",
        "                    \"pct1\": pct1,\n",
        "                    \"summary\": f\"%1={pct1:.1f}%\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Differential variables (only if at least 2 subclusters)\n",
        "    if len(groups) >= 2 and not (len(groups) == 1 and groups[0] == 0):\n",
        "        for col in num_cols:\n",
        "            group_vals = [df_p.loc[df_p[\"subcluster_final\"] == g, col] for g in groups]\n",
        "            try:\n",
        "                H, p = stats.kruskal(*group_vals)\n",
        "                eff = epsilon_squared_from_kruskal(float(H), k=len(groups), n=int(df_p.shape[0]))\n",
        "            except Exception:\n",
        "                H, p, eff = float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
        "            diff_rows.append(\n",
        "                {\n",
        "                    \"parent_cluster\": parent,\n",
        "                    \"variable\": col,\n",
        "                    \"type\": \"numeric\",\n",
        "                    \"test\": \"kruskal\",\n",
        "                    \"stat\": float(H),\n",
        "                    \"p_value\": float(p),\n",
        "                    \"effect_size\": float(eff),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        for col in (cat_cols + flag_cols):\n",
        "            ct = pd.crosstab(df_p[\"subcluster_final\"], df_p[col])\n",
        "            try:\n",
        "                chi2, p_chi2, dof, expected = stats.chi2_contingency(ct, correction=False)\n",
        "                expected_low = (expected < 5).any()\n",
        "                test_used = \"chi2\"\n",
        "                p_used = float(p_chi2)\n",
        "                if expected_low and ct.shape == (2, 2):\n",
        "                    test_used = \"fisher_exact\"\n",
        "                    _, p_f = stats.fisher_exact(ct.values)\n",
        "                    p_used = float(p_f)\n",
        "                eff = cramers_v_from_chi2(float(chi2), n=int(ct.values.sum()), r=ct.shape[0], c=ct.shape[1])\n",
        "                stat_val = float(chi2)\n",
        "            except Exception:\n",
        "                test_used, p_used, eff, stat_val = \"na\", float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
        "\n",
        "            diff_rows.append(\n",
        "                {\n",
        "                    \"parent_cluster\": parent,\n",
        "                    \"variable\": col,\n",
        "                    \"type\": \"categorical\" if col in cat_cols else \"flag\",\n",
        "                    \"test\": test_used,\n",
        "                    \"stat\": stat_val,\n",
        "                    \"p_value\": p_used,\n",
        "                    \"effect_size\": float(eff),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Narrative profiles (descriptive, no causality)\n",
        "    overall = df_p\n",
        "    for sc, sub in df_p.groupby(\"subcluster_final\"):\n",
        "        candidates = []\n",
        "\n",
        "        for col in num_cols:\n",
        "            delta = float(sub[col].median() - overall[col].median())\n",
        "            candidates.append(\n",
        "                (\n",
        "                    abs(delta),\n",
        "                    f\"{col}: median {sub[col].median():.1f} vs parent {overall[col].median():.1f} (Δ={delta:+.1f})\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for col in flag_cols:\n",
        "            delta = float(sub[col].mean() - overall[col].mean())\n",
        "            candidates.append(\n",
        "                (\n",
        "                    abs(delta),\n",
        "                    f\"{col}: %1 {sub[col].mean()*100:.1f}% vs parent {overall[col].mean()*100:.1f}% (Δ={delta*100:+.1f}pp)\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for col in cat_cols:\n",
        "            top_level = sub[col].value_counts(dropna=False).index[0]\n",
        "            p_sc = float((sub[col] == top_level).mean())\n",
        "            p_pa = float((overall[col] == top_level).mean())\n",
        "            delta = p_sc - p_pa\n",
        "            candidates.append(\n",
        "                (\n",
        "                    abs(delta),\n",
        "                    f\"{col}='{top_level}': {p_sc*100:.1f}% vs parent {p_pa*100:.1f}% (Δ={delta*100:+.1f}pp)\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "        highlights = [txt for _, txt in candidates[:3]]\n",
        "        narratives.append(\n",
        "            {\n",
        "                \"parent_cluster\": parent,\n",
        "                \"subcluster_final\": int(sc),\n",
        "                \"text\": \"Perfil descriptivo (no causal): \" + \"; \".join(highlights),\n",
        "            }\n",
        "        )\n",
        "\n",
        "perfil_subclusters = pd.DataFrame(profile_rows)\n",
        "perfil_subclusters_path = OUTPUT_DIR / \"perfil_subclusters.csv\"\n",
        "perfil_subclusters.to_csv(perfil_subclusters_path, index=False)\n",
        "\n",
        "print(\"Exported:\", perfil_subclusters_path)\n",
        "print(\"\\nPerfil_subclusters preview:\")\n",
        "print(perfil_subclusters.head(10))\n",
        "\n",
        "diff_df = pd.DataFrame(diff_rows)\n",
        "if not diff_df.empty:\n",
        "    print(\"\\nTop differential variables by parent cluster:\")\n",
        "    for parent in [1, 2]:\n",
        "        sub = diff_df[diff_df[\"parent_cluster\"] == parent].sort_values(\"p_value\")\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        print(f\"\\nParent cluster {parent}:\")\n",
        "        print(sub.head(10))\n",
        "\n",
        "narr_df = pd.DataFrame(narratives)\n",
        "print(\"\\nNarrative profiles:\")\n",
        "for parent in [1, 2]:\n",
        "    sub = narr_df[narr_df[\"parent_cluster\"] == parent].sort_values(\"subcluster_final\")\n",
        "    print(f\"\\nParent cluster {parent}:\")\n",
        "    for _, row in sub.iterrows():\n",
        "        print(f\"- subcluster_final={int(row['subcluster_final'])}: {row['text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bacc9bf",
      "metadata": {},
      "source": [
        "## 7) Prognostic validation within each parent cluster\n",
        "\n",
        "Within each parent cluster, we compare Kaplan–Meier curves by `subcluster_final` (only if ≥2 subclusters of sufficient size).\n",
        "\n",
        "We report:\n",
        "- Event-rate by subcluster\n",
        "- Global log-rank test (and pairwise with Bonferroni if >2 groups)\n",
        "- S(1y), S(3y), S(5y) when follow-up allows (365/1095/1825 days)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909322b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "Parent cluster 1: view_used_final=None | groups=[0] | groups_ok=[0]\n",
            "Not enough subclusters with sufficient size for KM/log-rank within this parent cluster.\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "Parent cluster 2: view_used_final=None | groups=[0] | groups_ok=[0]\n",
            "Not enough subclusters with sufficient size for KM/log-rank within this parent cluster.\n"
          ]
        }
      ],
      "source": [
        "# Survival utilities (no external survival libraries)\n",
        "\n",
        "def kaplan_meier_estimator(time: np.ndarray, event: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Step-wise KM survival curve (timeline, survival), no CI.\"\"\"\n",
        "    time = np.asarray(time, dtype=float)\n",
        "    event = np.asarray(event, dtype=int)\n",
        "\n",
        "    unique_event_times = np.unique(time[event == 1])\n",
        "    unique_event_times.sort()\n",
        "\n",
        "    surv = 1.0\n",
        "    timeline = [0.0]\n",
        "    survival = [1.0]\n",
        "\n",
        "    for t in unique_event_times:\n",
        "        at_risk = np.sum(time >= t)\n",
        "        if at_risk == 0:\n",
        "            continue\n",
        "        d = np.sum((time == t) & (event == 1))\n",
        "        surv *= (1.0 - d / at_risk)\n",
        "        timeline.append(float(t))\n",
        "        survival.append(float(surv))\n",
        "\n",
        "    tmax = float(np.max(time))\n",
        "    if timeline[-1] < tmax:\n",
        "        timeline.append(tmax)\n",
        "        survival.append(float(surv))\n",
        "\n",
        "    return np.asarray(timeline), np.asarray(survival)\n",
        "\n",
        "def km_survival_at(timeline: np.ndarray, survival: np.ndarray, t0: float) -> float:\n",
        "    \"\"\"Return S(t0) from a step KM curve (post-step).\"\"\"\n",
        "    if t0 < 0:\n",
        "        return float(\"nan\")\n",
        "    idx = np.searchsorted(timeline, t0, side=\"right\") - 1\n",
        "    idx = int(np.clip(idx, 0, len(survival) - 1))\n",
        "    return float(survival[idx])\n",
        "\n",
        "def logrank_test_multigroup(time: np.ndarray, event: np.ndarray, group: np.ndarray) -> tuple[float, float, int]:\n",
        "    \"\"\"Multi-group log-rank test with chi-square approximation.\"\"\"\n",
        "    time = np.asarray(time, dtype=float)\n",
        "    event = np.asarray(event, dtype=int)\n",
        "    group = np.asarray(group)\n",
        "\n",
        "    groups = np.unique(group)\n",
        "    k = len(groups)\n",
        "    if k < 2:\n",
        "        return float(\"nan\"), float(\"nan\"), 0\n",
        "\n",
        "    group_to_idx = {g: i for i, g in enumerate(groups)}\n",
        "    g_idx = np.array([group_to_idx[g] for g in group], dtype=int)\n",
        "\n",
        "    event_times = np.unique(time[event == 1])\n",
        "    event_times.sort()\n",
        "\n",
        "    O = np.zeros(k, dtype=float)\n",
        "    E = np.zeros(k, dtype=float)\n",
        "    V = np.zeros((k, k), dtype=float)\n",
        "\n",
        "    for t in event_times:\n",
        "        at_risk = time >= t\n",
        "        n = int(at_risk.sum())\n",
        "        if n <= 1:\n",
        "            continue\n",
        "        d = int(np.sum((time == t) & (event == 1)))\n",
        "        if d == 0:\n",
        "            continue\n",
        "\n",
        "        n_g = np.zeros(k, dtype=float)\n",
        "        d_g = np.zeros(k, dtype=float)\n",
        "        for i in range(k):\n",
        "            mask = g_idx == i\n",
        "            n_g[i] = float(np.sum(at_risk & mask))\n",
        "            d_g[i] = float(np.sum((time == t) & (event == 1) & mask))\n",
        "\n",
        "        O += d_g\n",
        "        E += d * (n_g / n)\n",
        "\n",
        "        factor = d * (n - d) / (n - 1)\n",
        "        if factor <= 0:\n",
        "            continue\n",
        "        p = n_g / n\n",
        "        V += factor * (np.diag(p) - np.outer(p, p))\n",
        "\n",
        "    U = O - E\n",
        "    dof = k - 1\n",
        "    U_r = U[:-1]\n",
        "    V_r = V[:-1, :-1]\n",
        "    stat = float(U_r.T @ np.linalg.pinv(V_r) @ U_r)\n",
        "    p_value = float(stats.chi2.sf(stat, dof))\n",
        "    return stat, p_value, dof\n",
        "\n",
        "survival_results = {}\n",
        "\n",
        "timepoints = [365, 1095, 1825]\n",
        "\n",
        "for parent in [1, 2]:\n",
        "    df_p = df_sub[df_sub[PARENT_CLUSTER] == parent].copy()\n",
        "    min_size = int(subclustering_results[parent][\"min_cluster_size\"])\n",
        "    groups = sorted(df_p[\"subcluster_final\"].unique().tolist())\n",
        "\n",
        "    # Only consider groups with enough samples\n",
        "    groups_ok = [g for g in groups if int((df_p[\"subcluster_final\"] == g).sum()) >= min_size]\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 90)\n",
        "    print(f\"Parent cluster {parent}: view_used_final={view_used_final[parent]} | groups={groups} | groups_ok={groups_ok}\")\n",
        "\n",
        "    if len(groups_ok) < 2:\n",
        "        print(\"Not enough subclusters with sufficient size for KM/log-rank within this parent cluster.\")\n",
        "        survival_results[parent] = {\"status\": \"skipped_insufficient_groups\"}\n",
        "        continue\n",
        "\n",
        "    # Event counts and rates\n",
        "    rows = []\n",
        "    for g in groups_ok:\n",
        "        sub = df_p[df_p[\"subcluster_final\"] == g]\n",
        "        n = int(sub.shape[0])\n",
        "        e = int(sub[TARGET_EVENT].sum())\n",
        "        er = float(sub[TARGET_EVENT].mean())\n",
        "        rows.append({\"subcluster_final\": int(g), \"n\": n, \"events\": e, \"event_rate\": er})\n",
        "    event_table = pd.DataFrame(rows).sort_values(\"subcluster_final\")\n",
        "    print(\"\\nEvent table:\")\n",
        "    print(event_table)\n",
        "\n",
        "    total_events = int(df_p[TARGET_EVENT].sum())\n",
        "    if total_events < 10:\n",
        "        print(\"WARNING: Low number of events in this parent cluster -> low power for survival separation.\")\n",
        "\n",
        "    # KM plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    km_curves = {}\n",
        "    for g in groups_ok:\n",
        "        sub = df_p[df_p[\"subcluster_final\"] == g]\n",
        "        t, s = kaplan_meier_estimator(sub[TARGET_TIME].to_numpy(), sub[TARGET_EVENT].to_numpy())\n",
        "        km_curves[int(g)] = (t, s)\n",
        "        plt.step(t, s, where=\"post\", label=f\"Subcluster {g} (n={sub.shape[0]})\")\n",
        "\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Survival probability\")\n",
        "    plt.title(f\"Kaplan–Meier within parent cluster {parent}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Log-rank\n",
        "    mask_ok = df_p[\"subcluster_final\"].isin(groups_ok).to_numpy()\n",
        "    time_ok = df_p.loc[mask_ok, TARGET_TIME].to_numpy()\n",
        "    event_ok = df_p.loc[mask_ok, TARGET_EVENT].to_numpy()\n",
        "    group_ok = df_p.loc[mask_ok, \"subcluster_final\"].to_numpy()\n",
        "\n",
        "    lr_chi2, lr_p, lr_dof = logrank_test_multigroup(time_ok, event_ok, group_ok)\n",
        "    print(f\"Global log-rank: chi2={lr_chi2:.3f}, dof={lr_dof}, p={lr_p:.4g}\")\n",
        "\n",
        "    pairwise = None\n",
        "    if len(groups_ok) > 2:\n",
        "        pairs = list(itertools.combinations(groups_ok, 2))\n",
        "        m = len(pairs)\n",
        "        rows = []\n",
        "        for a, b in pairs:\n",
        "            mask = df_p[\"subcluster_final\"].isin([a, b]).to_numpy()\n",
        "            chi2_ab, p_ab, _ = logrank_test_multigroup(\n",
        "                df_p.loc[mask, TARGET_TIME].to_numpy(),\n",
        "                df_p.loc[mask, TARGET_EVENT].to_numpy(),\n",
        "                df_p.loc[mask, \"subcluster_final\"].to_numpy(),\n",
        "            )\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"a\": int(a),\n",
        "                    \"b\": int(b),\n",
        "                    \"chi2\": float(chi2_ab),\n",
        "                    \"p_value\": float(p_ab),\n",
        "                    \"p_bonferroni\": float(min(p_ab * m, 1.0)),\n",
        "                }\n",
        "            )\n",
        "        pairwise = pd.DataFrame(rows).sort_values(\"p_value\")\n",
        "        print(\"\\nPairwise log-rank (Bonferroni):\")\n",
        "        print(pairwise)\n",
        "\n",
        "    # S(1y), S(3y), S(5y) if follow-up allows\n",
        "    surv_tp_rows = []\n",
        "    for g in groups_ok:\n",
        "        sub = df_p[df_p[\"subcluster_final\"] == g]\n",
        "        max_fu = float(sub[TARGET_TIME].max())\n",
        "        t, s = km_curves[int(g)]\n",
        "        row = {\"parent_cluster\": parent, \"subcluster_final\": int(g), \"max_followup_days\": max_fu}\n",
        "        for tp in timepoints:\n",
        "            if max_fu >= tp:\n",
        "                row[f\"S_{tp}d\"] = km_survival_at(t, s, tp)\n",
        "            else:\n",
        "                row[f\"S_{tp}d\"] = float(\"nan\")\n",
        "        surv_tp_rows.append(row)\n",
        "\n",
        "    surv_tp = pd.DataFrame(surv_tp_rows)\n",
        "    print(\"\\nSurvival probabilities at fixed timepoints (NaN if insufficient follow-up):\")\n",
        "    print(surv_tp)\n",
        "\n",
        "    survival_results[parent] = {\n",
        "        \"status\": \"ok\",\n",
        "        \"groups_ok\": groups_ok,\n",
        "        \"event_table\": event_table,\n",
        "        \"logrank_global\": {\"chi2\": float(lr_chi2), \"p_value\": float(lr_p), \"dof\": int(lr_dof)},\n",
        "        \"logrank_pairwise\": pairwise,\n",
        "        \"survival_timepoints\": surv_tp,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee55cf1d",
      "metadata": {},
      "source": [
        "## 8) Interpretation and final decision\n",
        "\n",
        "We summarize, for each parent cluster:\n",
        "- Whether View A has robust substructure\n",
        "- Whether View B has robust substructure\n",
        "- Whether there is survival separation\n",
        "- Which variables define the subgroups\n",
        "\n",
        "Conclusion rule-of-thumb:\n",
        "- If View A is robust → report as **clinical subphenotypes**.\n",
        "- If only View B is robust and top drivers are missingness/flags → report as **data availability profiles**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2624f73c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final summary by parent cluster:\n",
            "   parent_cluster  n_parent  view_A_selected  view_A_K  view_B_selected  \\\n",
            "0               1        86            False       NaN            False   \n",
            "1               2        61            False       NaN            False   \n",
            "\n",
            "   view_B_K view_used_final  logrank_p_within_parent top_drivers  \\\n",
            "0       NaN            none                      NaN               \n",
            "1       NaN            none                      NaN               \n",
            "\n",
            "   flag_fraction_in_top8                                     interpretation  \n",
            "0                    0.0  No robust substructure detected (subcluster_fi...  \n",
            "1                    0.0  No robust substructure detected (subcluster_fi...  \n",
            "\n",
            "Exports:\n",
            "- df_con_subclusters_csv: cluster/df_con_subclusters.csv\n",
            "- perfil_subclusters_csv: cluster/perfil_subclusters.csv\n"
          ]
        }
      ],
      "source": [
        "def top_differential_variables(diff_df: pd.DataFrame, parent: int, top_n: int = 5) -> pd.DataFrame:\n",
        "    if diff_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    sub = diff_df[diff_df[\"parent_cluster\"] == parent].sort_values(\"p_value\")\n",
        "    return sub.head(top_n)\n",
        "\n",
        "summary_rows = []\n",
        "for parent in [1, 2]:\n",
        "    sel_A = subclustering_results[parent][\"views\"][\"A\"][\"selected\"]\n",
        "    sel_B = subclustering_results[parent][\"views\"][\"B\"][\"selected\"]\n",
        "    used = view_used_final[parent]\n",
        "\n",
        "    top_vars = top_differential_variables(diff_df, parent=parent, top_n=8)\n",
        "    top_vars_list = top_vars[\"variable\"].tolist() if not top_vars.empty else []\n",
        "    n_flags_in_top = sum(v in flag_cols for v in top_vars_list)\n",
        "    flag_fraction = (n_flags_in_top / len(top_vars_list)) if top_vars_list else 0.0\n",
        "\n",
        "    surv = survival_results.get(parent, {})\n",
        "    lr_p = float(surv.get(\"logrank_global\", {}).get(\"p_value\", np.nan))\n",
        "\n",
        "    interpretation = \"\"\n",
        "    if used == \"A\":\n",
        "        interpretation = \"Clinical subphenotypes (View A selected).\"\n",
        "    elif used == \"B\":\n",
        "        if flag_fraction >= 0.5:\n",
        "            interpretation = \"Likely data availability / missingness-driven (View B only; many top drivers are flags).\"\n",
        "        else:\n",
        "            interpretation = \"Operational profile (View B selected), but drivers are not dominated by flags.\"\n",
        "    else:\n",
        "        interpretation = \"No robust substructure detected (subcluster_final=0).\"\n",
        "\n",
        "    summary_rows.append(\n",
        "        {\n",
        "            \"parent_cluster\": parent,\n",
        "            \"n_parent\": subclustering_results[parent][\"n_parent\"],\n",
        "            \"view_A_selected\": sel_A is not None,\n",
        "            \"view_A_K\": int(sel_A[\"K\"]) if sel_A else np.nan,\n",
        "            \"view_B_selected\": sel_B is not None,\n",
        "            \"view_B_K\": int(sel_B[\"K\"]) if sel_B else np.nan,\n",
        "            \"view_used_final\": used if used else \"none\",\n",
        "            \"logrank_p_within_parent\": lr_p,\n",
        "            \"top_drivers\": \", \".join(top_vars_list[:5]),\n",
        "            \"flag_fraction_in_top8\": flag_fraction,\n",
        "            \"interpretation\": interpretation,\n",
        "        }\n",
        "    )\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(\"Final summary by parent cluster:\")\n",
        "print(summary_df)\n",
        "\n",
        "# Collect results object\n",
        "results = {\n",
        "    \"RANDOM_STATE\": RANDOM_STATE,\n",
        "    \"input_path\": str(INPUT_PATH),\n",
        "    \"output_dir\": str(OUTPUT_DIR),\n",
        "    \"subclustering\": subclustering_results,\n",
        "    \"view_used_final\": view_used_final,\n",
        "    \"df_sub\": df_sub,\n",
        "    \"perfil_subclusters\": perfil_subclusters,\n",
        "    \"diff_subclusters\": diff_df,\n",
        "    \"narratives\": narr_df,\n",
        "    \"survival_results\": survival_results,\n",
        "    \"summary\": summary_df,\n",
        "    \"exports\": {\n",
        "        \"df_con_subclusters_csv\": str(OUTPUT_DIR / \"df_con_subclusters.csv\"),\n",
        "        \"perfil_subclusters_csv\": str(OUTPUT_DIR / \"perfil_subclusters.csv\"),\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"\\nExports:\")\n",
        "for k, v in results[\"exports\"].items():\n",
        "    print(f\"- {k}: {v}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
